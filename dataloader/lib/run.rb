require 'json'
require 'logger'
require_relative 'firebase'
require_relative 'location_parser'
require_relative 'schedule_parser'

def jsonify(hashmap)
  JSON.generate(hashmap, space: ' ', indent: '  ', object_nl: "\n" )
end

logger = Logger.new(STDOUT)
logger.level = Logger::INFO

firestore = PoolfinderFirestore.new

locations = LocationParser.parse('../static_data/pools.xml')
all_location_ids = LocationParser.parse('../static_data/pools.xml').map {|pool| pool[:id]}
File.open('../../client/src/js/locations.js', 'w') do |file|
  file.write("// THIS FILE IS AUTOMATICALLY GENERATED - DO NOT EDIT IT DIRECTLY!\n")
  file.write("export default #{jsonify(locations)};")
  # TODO: We can write the time we pulled the data into this file and display it on the frontend
end

schedule_urls_to_scrape = [
  'https://www.toronto.ca/data/parks/prd/swimming/dropin/leisure/index.html',
  'https://www.toronto.ca/data/parks/prd/swimming/dropin/family/index.html',
  'https://www.toronto.ca/data/parks/prd/swimming/dropin/female/index.html',
  'https://www.toronto.ca/data/parks/prd/swimming/dropin/lane/index.html'

]

total_schedule =
  schedule_urls_to_scrape
  .map do |url|
    logger.info("Scraping url \"#{url}\"...")
    ScheduleParser.parse_swim_times(url)
  end
  .flatten

# Log any non-existant locations
total_schedule
  .map { |location| location[:location_id]}
  .uniq
  .reject { |id| all_location_ids.include?(id) }
  .each { |id| logger.warn("Location with ID [#{id}] not in locations list") }

locations
  .map do |location|
    location[:schedule] =
      total_schedule
      .select { |s| s[:location_id] == location[:id] }
      .map do |s|
        {
          from: s[:from],
          to: s[:to],
          activity: s[:activity],
        }
      end
    location
  end
  .each { |location| firestore.create_or_update_location(location)}
